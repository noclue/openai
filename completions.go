package openai

import (
	"context"
	"fmt"
)

var completionsPath = fmt.Sprintf("%v/%v/completions", basePath, apiVersion)

type LogitBias map[string]int8

type CompletionsRequest struct {
	// The model to use for generating completions.
	Model string `json:"model"`
	// The text prompt to generate completions for.
	Prompt string `json:"prompt"`
	// The suffix that comes after a completion of inserted text.
	Suffix string `json:"suffix,omitempty"`
	// The maximum number of tokens (words or word pieces) to generate in the completion.
	// If not specified, the default is 50.
	MaxTokens *int `json:"max_tokens,omitempty"`
	// Controls the randomness of the model's output.
	// Higher values mean the model will be more random.
	// If not specified, the default is 1.
	Temperature *float64 `json:"temperature,omitempty"`
	// Controls the proportion of the mass of the probability distribution that is concentrated in the top-k candidates.
	// Higher values means the model will be more deterministic.
	// If not specified, the default is 1.
	TopP *float64 `json:"top_p,omitempty"`
	// The number of completions to generate. If not specified, the default is 1.
	N *int `json:"n,omitempty"`
	// Whether to stream back partial progress. If set, tokens will be sent as
	// data-only server-sent events as they become available, with the stream
	// terminated by a data: [DONE] message.
	Stream *bool `json:"stream,omitempty"`
	// Include the log probabilities on the logprobs most likely tokens, as
	// well the chosen tokens. For example, if logprobs is 5, the API will
	// return a list of the 5 most likely tokens. The API will always return
	// the logprob of the sampled token, so there may be up to logprobs+1
	// elements in the response.
	//
	// The maximum value for logprobs is 5. If you need more than this, please
	// contact OpenAI through their Help center and describe your use case.
	Logprobs *int `json:"logprobs,omitempty"`
	// Whether to return the prompt in addition to the completion.
	Echo *bool `json:"echo,omitempty"`
	// A string that, if encountered by the model, will cause it to stop generating completions.
	// If not specified, the default is null.
	Stop string `json:"stop,omitempty"`
	// Controls the penalty applied to words that are more present in the prompt.
	// Higher values mean more penalty.
	// If not specified, the default is 0.
	PresencePenalty *float64 `json:"presence_penalty,omitempty"`
	// Controls the penalty applied to words that are more frequent in the training data.
	// Higher values mean more penalty.
	// If not specified, the default is 0.
	FrequencyPenalty *float64 `json:"frequency_penalty,omitempty"`
	// Controls the number of candidates to consider when filtering the top-k candidates.
	// Higher values means the model will be more deterministic.
	// If not specified, the default is 1.
	BestOf *int `json:"best_of,omitempty"`
	// Modify the likelihood of specified tokens appearing in the completion.
	//
	// Accepts a json object that maps tokens (specified by their token ID in
	// the GPT tokenizer) to an associated bias value from -100 to 100. You can
	// use this tokenizer tool (which works for both GPT-2 and GPT-3) to
	// convert text to token IDs. Mathematically, the bias is added to the
	// logits generated by the model prior to sampling. The exact effect will
	// vary per model, but values between -1 and 1 should decrease or increase
	// likelihood of selection; values like -100 or 100 should result in a ban
	// or exclusive selection of the relevant token.
	//
	// As an example, you can pass {"50256": -100} to prevent the <|endoftext|>
	// token from being generated.
	LogitBias *LogitBias `json:"logit_bias,omitempty"`
	// A unique identifier representing your end-user, which can help OpenAI
	// to monitor and detect abuse. See [End User Ids] for details
	//
	// [End User Ids]: https://beta.openai.com/docs/guides/safety-best-practices/end-user-ids
	User string `json:"user,omitempty"`
}

// TODO add the methods to the client

// Choice is a single completion from the Completions endpoint. See
// https://beta.openai.com/docs/api-reference/completions/create-completion
type Choice struct {
	// The text of the completion.
	Text string `json:"text"`
	// The index of the completion in the request.
	Index int `json:"index"`
	// The log probability of the completion.
	Logprobs map[string]float64 `json:"logprobs"`
	// The reason the request was finished. One of: "stop", "length", "time", "interrupted", "api", "model", "engine", "unknown"
	FinishReason string `json:"finish_reason"`
}

// Usage is the usage information per request to OpenAI.
type Usage struct {
	PropmtTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// CompletionsResponse is the response from the Completions endpoint. See
// https://beta.openai.com/docs/api-reference/completions/create-completion
type CompletionsResponse struct {
	// The ID of the completion request.
	Id      string `json:"id"`
	Object  string `json:"object"`
	Created int64  `json:"created"`
	// The model used to generate the completion.
	Model   string   `json:"model"`
	Choices []Choice `json:"choices"`
	Usage   Usage    `json:"usage"`
}

func (c *openAI) CreateCompletion(ctx context.Context, req *CompletionsRequest) (*CompletionsResponse, error) {
	var resp CompletionsResponse
	err := c.makeJSONRequest(ctx, completionsPath, req, &resp)
	if err != nil {
		return nil, err
	}
	return &resp, nil
}
